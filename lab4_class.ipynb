{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Practical machine learning and deep learning. Lab 4\n\n# Many-to-many NLP task.\n\n# [Competition](https://www.kaggle.com/t/afa89356762e438cad5f04bf0e23f3ce)\n\n## Goal\n\nYour goal is to implement Neural Network for tagging the part-of-speech entities.\n\n## Submission\n\nSubmission format is described at competition page.\n\n> Remember, you can use any structure of the solution. The template classes/function in this file is just the tip for you. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:36.008372Z","iopub.execute_input":"2023-09-21T14:05:36.008757Z","iopub.status.idle":"2023-09-21T14:05:36.013977Z","shell.execute_reply.started":"2023-09-21T14:05:36.008726Z","shell.execute_reply":"2023-09-21T14:05:36.012659Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"markdown","source":"## Data reading and preprocessing","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/pmldl-week4-many-to-many-nlp-task/train.csv')\ntest = pd.read_csv('/kaggle/input/pmldl-week4-many-to-many-nlp-task/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:36.033085Z","iopub.execute_input":"2023-09-21T14:05:36.033658Z","iopub.status.idle":"2023-09-21T14:05:36.726379Z","shell.execute_reply.started":"2023-09-21T14:05:36.033624Z","shell.execute_reply":"2023-09-21T14:05:36.725312Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:36.728602Z","iopub.execute_input":"2023-09-21T14:05:36.729090Z","iopub.status.idle":"2023-09-21T14:05:36.742116Z","shell.execute_reply.started":"2023-09-21T14:05:36.729053Z","shell.execute_reply":"2023-09-21T14:05:36.740816Z"},"trusted":true},"execution_count":156,"outputs":[{"execution_count":156,"output_type":"execute_result","data":{"text/plain":"   sentence_id  entity_id entity   tag\n0            0          0     It  PRON\n1            0          1     is  VERB\n2            0          2   true   ADJ\n3            0          3   that   ADP\n4            0          4    his   DET","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>entity_id</th>\n      <th>entity</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>It</td>\n      <td>PRON</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>is</td>\n      <td>VERB</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>true</td>\n      <td>ADJ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>3</td>\n      <td>that</td>\n      <td>ADP</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>4</td>\n      <td>his</td>\n      <td>DET</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:36.744377Z","iopub.execute_input":"2023-09-21T14:05:36.744909Z","iopub.status.idle":"2023-09-21T14:05:36.758321Z","shell.execute_reply.started":"2023-09-21T14:05:36.744864Z","shell.execute_reply":"2023-09-21T14:05:36.757221Z"},"trusted":true},"execution_count":157,"outputs":[{"execution_count":157,"output_type":"execute_result","data":{"text/plain":"   id  sentence_id  entity_id     entity\n0   0            0          0         In\n1   1            0          1    another\n2   2            0          2    setback\n3   3            0          3  yesterday\n4   4            0          4          ,","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentence_id</th>\n      <th>entity_id</th>\n      <th>entity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>In</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>another</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>setback</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>yesterday</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>,</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"First, let's divide dataset on train and validation. And split the dataframe according to random split.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nVALIDATION_RATIO = 0.2\ntrain_split, val_split = train_test_split(range(train['sentence_id'].max()), test_size=VALIDATION_RATIO, random_state=420)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:36.762161Z","iopub.execute_input":"2023-09-21T14:05:36.762713Z","iopub.status.idle":"2023-09-21T14:05:36.790341Z","shell.execute_reply.started":"2023-09-21T14:05:36.762648Z","shell.execute_reply":"2023-09-21T14:05:36.789371Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"markdown","source":"And then split the original dataframe by ids that we splitted.","metadata":{}},{"cell_type":"code","source":"train_dataframe = train[train['sentence_id'].isin(train_split)]\nval_dataframe = train[train['sentence_id'].isin(val_split)]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:36.792617Z","iopub.execute_input":"2023-09-21T14:05:36.793324Z","iopub.status.idle":"2023-09-21T14:05:36.883516Z","shell.execute_reply.started":"2023-09-21T14:05:36.793288Z","shell.execute_reply":"2023-09-21T14:05:36.882436Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"code","source":"pos_tags = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']\ncat2idx = {tag: i for i, tag in enumerate(pos_tags)}\nidx2cat = {v: k for k, v in cat2idx.items()}\n\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:36.885284Z","iopub.execute_input":"2023-09-21T14:05:36.886002Z","iopub.status.idle":"2023-09-21T14:05:36.893061Z","shell.execute_reply.started":"2023-09-21T14:05:36.885965Z","shell.execute_reply":"2023-09-21T14:05:36.891957Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"markdown","source":"For working with datasets more efficiently, let's create separate classes for datasets. \n\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\ntorch.manual_seed(420)\nfrom torchtext.vocab import build_vocab_from_iterator\n\n\nclass PosTaggingDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe: pd.DataFrame, vocab = None, max_size=100):\n        self.dataframe = dataframe\n        self._preprocess()\n        self.vocab = vocab or self._create_vocab()\n\n    def _preprocess(self):\n        # fill missing values in entities\n        self.dataframe['entity'] = self.dataframe['entity'].fillna(\"\")\n\n        # Fill missing tag to `other` - `X`\n        self.dataframe[self.dataframe['tag'].isna()]['tag'] = 'X'\n\n        # Clean entities column\n        self.dataframe['entity'] = self.dataframe['entity'].str.lower()\n        \n        # Split the dataset, so that we will have \n        # full sentences and full tags by the same index\n        self.dataframe.sort_values(by=['sentence_id', 'entity_id'], ascending=[True, True])\n\n        self.sentences = list(self.dataframe.groupby('sentence_id')['entity'].agg(list).reset_index()['entity'])\n        self.sentences = [\" \".join(l) for l in self.sentences]\n        \n        self.tags = list(self.dataframe.groupby('sentence_id')['tag'].agg(list).reset_index()['tag'])\n    \n    def _create_vocab(self):\n        # creates vocabulary that is used for encoding \n        # the sequence of tokens (splitted sentence)        \n        vocab = build_vocab_from_iterator(\n            [sentence.split() for sentence in self.sentences],\n            min_freq=1,\n            specials=special_symbols,\n            special_first=True\n        )\n        vocab.set_default_index(UNK_IDX)\n        return vocab\n\n    def _get_sentence(self, index: int) -> list:\n        # retrieves sentence from dataset by index\n        sent = self.sentences[index]\n        return self.vocab(sent.split())\n\n    def _get_labels(self, index: int) -> list:\n        # retrieves tags from dataset by index\n        tags = self.tags[index]\n        return [cat2idx[tag] for tag in tags]\n\n    def __getitem__(self, index) -> tuple[list, list]:\n        return self._get_sentence(index), self._get_labels(index)\n    \n    def __len__(self) -> int:\n        return len(self.sentences)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:36.897005Z","iopub.execute_input":"2023-09-21T14:05:36.898233Z","iopub.status.idle":"2023-09-21T14:05:36.918018Z","shell.execute_reply.started":"2023-09-21T14:05:36.898194Z","shell.execute_reply":"2023-09-21T14:05:36.916987Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"code","source":"# Create train dataset\ntrain_dataset = PosTaggingDataset(train_dataframe)\nval_dataset = PosTaggingDataset(val_dataframe)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:36.920825Z","iopub.execute_input":"2023-09-21T14:05:36.921831Z","iopub.status.idle":"2023-09-21T14:05:43.631209Z","shell.execute_reply.started":"2023-09-21T14:05:36.921804Z","shell.execute_reply":"2023-09-21T14:05:43.630142Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"markdown","source":"And now we are able to create dataloader faster, because we created torch datasets","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\nbatch_size = 128\nmax_size = 30\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndef collate_batch(batch: list):\n    # Collate list of samples into tensor batch\n    # As an input we have list of pair from dataset:\n    # [([ent1, ent2, ...], [tag1, tag2, ...]), ([ent1, ent2, ...], [tag1, tag2, ...]), ...]\n    # as an output, we want to have tensor of entities and tensor of tags \n    sentences_batch, postags_batch = [], []\n\n    for _sent, _postags in batch:\n        _sent = _sent[:max_size]\n    \n        _sent = [PAD_IDX] * ((max_size-len(_sent)) if (len(_sent) < max_size) else 0) + _sent\n        _postags = _postags[:max_size]\n        _postags = [cat2idx['X']] * ((max_size-len(_postags)) if (len(_postags) < max_size) else 0) + _postags\n        sentences_batch.append(_sent); postags_batch.append(_postags)\n        \n\n    # Remember, that if we want to perform many to many mapping with our network with recurrent units, \n    # we want pass first item from all sequences as first input, thus\n    # we want to have tensor with shape (max_size, ...., batch_size)\n    sentences_batch = torch.tensor(sentences_batch).T\n    postags_batch = torch.tensor(postags_batch).T\n    \n    return sentences_batch.to(device), postags_batch.to(device)\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=128, shuffle=True, collate_fn=collate_batch\n)\nval_dataloader = DataLoader(\n    val_dataset, batch_size=128, shuffle=False, collate_fn=collate_batch\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:43.632839Z","iopub.execute_input":"2023-09-21T14:05:43.633195Z","iopub.status.idle":"2023-09-21T14:05:43.715421Z","shell.execute_reply.started":"2023-09-21T14:05:43.633162Z","shell.execute_reply":"2023-09-21T14:05:43.714497Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"# just to check that all shapes are correct\n\nfor batch in train_dataloader:\n    inp, out = batch\n    print(inp.shape)\n    print(out.shape)\n    print(inp[:,0])\n    print(out[:,0])\n    break","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:43.719995Z","iopub.execute_input":"2023-09-21T14:05:43.720297Z","iopub.status.idle":"2023-09-21T14:05:43.740621Z","shell.execute_reply.started":"2023-09-21T14:05:43.720272Z","shell.execute_reply":"2023-09-21T14:05:43.739724Z"},"trusted":true},"execution_count":164,"outputs":[{"name":"stdout","text":"torch.Size([30, 128])\ntorch.Size([30, 128])\ntensor([    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n            1,     1,     1,     1,     1, 22908,    14,     4, 43730,     6],\n       device='cuda:0')\ntensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n        11, 11, 11, 11, 11, 11, 11,  5,  9,  4,  5, 10], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Creating the network\n\nFor the many-to-many or seq2seq netoworks, we want to have recurrent units in the network. This gives the ability for network to learn the hidden features and pass the knowledge from one token to other. \n\n### Embeddings\n\nFor embeddings you can use `nn.Embedding` for creating your own features or use pretrained embedding (like GloVe or FastText or Bert).\n\n### Recurrent\n\nFor processing sequences you can use recurrent units like `LSTM`.\n\n### Linear\n\nAdd simple nn.Linear. ~~This is basic stuff what do you want~~\n\n### Regularization\n\nRemeber to set up Dropout and Batch Normalization for regularization purposes.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass POSTagger(nn.Module):\n    def __init__(self,  vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, dropout_prob):\n        \n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True)\n        self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)\n        self.linear = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.Dropout(p=dropout_prob),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_classes))\n        self.dropout = nn.Dropout(p=dropout_prob)\n        \n        \n    def forward(self, text):\n\n        # text shape= [sent len, batch size]\n        emb = self.emb(text)\n#         print(f'emb shape: {emb.size()}')\n    \n        lstm = self.lstm(emb)[0]\n#         print(f'lstm shape: {lstm.size()}')\n        \n#         drop = self.dropout(lstm)\n#         print(f'drop shape: {drop.size()}')\n        \n#         norm = self.batch_norm(drop.permute(0, 2, 1)).permute(0, 2, 1)\n#         print(f'norm shape: {norm.size()}')\n        \n        predictions = self.linear(lstm)\n#         print(f'predictions shape: {predictions.size()}')\n        \n        # predictions shape = [sent len, batch size, output dim]\n        return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:43.742114Z","iopub.execute_input":"2023-09-21T14:05:43.742513Z","iopub.status.idle":"2023-09-21T14:05:43.751758Z","shell.execute_reply.started":"2023-09-21T14:05:43.742481Z","shell.execute_reply":"2023-09-21T14:05:43.750531Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"m = POSTagger(len(train_dataset.vocab), 32, 10, 5, 12, 0.2).to('cuda')\ni = torch.tensor(np.random.randint(0, 12, size=(120,128))).to('cuda')\no = m(i)\nprint(torch.max(o, axis=2)[1][:,0])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:43.753102Z","iopub.execute_input":"2023-09-21T14:05:43.753911Z","iopub.status.idle":"2023-09-21T14:05:43.793715Z","shell.execute_reply.started":"2023-09-21T14:05:43.753877Z","shell.execute_reply":"2023-09-21T14:05:43.792704Z"},"trusted":true},"execution_count":166,"outputs":[{"name":"stdout","text":"tensor([10,  2,  2,  2,  2,  2,  2,  2,  2, 10,  2,  2,  2,  2,  2,  2,  2,  2,\n         2,  2,  2,  2,  2,  2,  2,  2,  2, 10, 10, 10,  2,  2,  2, 10, 10,  2,\n         2, 10,  2,  2,  2, 10, 10,  2, 10,  2,  2, 10, 10,  2,  2,  2, 10, 10,\n         2, 10,  2,  2,  2, 10, 10,  2,  2, 10,  2,  2,  2,  2,  2, 10,  2,  2,\n         2, 10, 10, 10, 10, 10,  2,  2,  2,  2,  2,  2,  2,  2, 10,  2, 10,  2,\n         2, 10,  2,  2,  2,  2,  2,  2, 10,  2, 10,  2, 10, 10,  2, 10,  2,  2,\n         2,  2,  2, 10, 10,  2,  2, 10,  2, 10,  2,  2], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training\n\nAs for training you should take into account that the shape of your output and shape of the labels. Perform required transformations and use loss function that fits your task.\n\n> Do not forget about tqdm and logging, you want normal training not some unreadable ~~sht~~ logs. ","metadata":{}},{"cell_type":"code","source":"from tqdm.autonotebook import tqdm\n\ndef train_one_epoch(\n    model,\n    loader,\n    optimizer,\n    loss_fn,\n    epoch_num=-1\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Epoch {epoch}: train\",\n        leave=True,\n    )\n    model.train()\n    train_loss = 0.0\n    total = 0\n    for i, batch in loop:\n        texts, labels = batch\n        \n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward pass and loss calculation\n        outputs = model(texts) # max_size, batch_size, classes\n        \n#         print(labels[-20:,0])\n#         print(torch.max(outputs, axis=2)[1][-20:,0])\n        \n        loss = loss_fn(outputs.permute(0,2,1), labels)\n        \n        # backward pass\n        loss.backward()\n\n        # optimizer run\n        optimizer.step()\n\n        train_loss += loss.item()\n        loop.set_postfix({\"loss\": train_loss/((i+1) * labels.size(0) * labels.size(1))})\n\n\ndef val_one_epoch(\n    model,\n    loader,\n    loss_fn,\n    epoch_num=-1,\n    best_so_far=0.0,\n    ckpt_path='best.pt'\n):\n    \n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Epoch {epoch}: val\",\n        leave=True,\n    )\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        model.eval()  # evaluation mode\n        for i, batch in loop:\n            texts, labels = batch\n        \n            # forward pass and loss calculation\n            outputs = model(texts)\n    \n            loss = loss_fn(outputs.permute(0,2,1), labels)\n            \n            _, predicted = torch.max(outputs, axis=2) # max_size, batch_size, classes\n            \n#             print(labels[-20:,0])\n#             print(torch.max(outputs, axis=2)[1][-20:,0])\n            \n            total += labels.size(0) * labels.size(1)\n        \n            correct += (predicted == labels).sum().item()\n#             print(predicted[5:10, 5], '\\n', labels[5:10, 5])\n\n            val_loss += loss.item()\n\n            loop.set_postfix({\"loss\": val_loss/total, \"acc\": correct / total})\n        \n        if correct / total > best_so_far:\n            torch.save(model.state_dict(), ckpt_path)\n            print('model saved')\n            return correct / total\n\n    return best_so_far","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:43.795262Z","iopub.execute_input":"2023-09-21T14:05:43.795591Z","iopub.status.idle":"2023-09-21T14:05:43.808916Z","shell.execute_reply.started":"2023-09-21T14:05:43.795560Z","shell.execute_reply":"2023-09-21T14:05:43.807754Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"INPUT_DIM = len(train_dataset.vocab)\nOUTPUT_DIM = len(pos_tags)\n\nmodel = POSTagger(INPUT_DIM, 128, 128, 2, OUTPUT_DIM, 0.25).to(device)\n\noptimizer = torch.optim.Adam(model.parameters())\nloss_fn = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:43.810319Z","iopub.execute_input":"2023-09-21T14:05:43.810970Z","iopub.status.idle":"2023-09-21T14:05:43.887581Z","shell.execute_reply.started":"2023-09-21T14:05:43.810934Z","shell.execute_reply":"2023-09-21T14:05:43.886577Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"best_so_far = -float('inf')\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch_num=epoch)\n    best_so_far = val_one_epoch(model, val_dataloader, loss_fn, epoch, best_so_far=best_so_far)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:05:43.888866Z","iopub.execute_input":"2023-09-21T14:05:43.889225Z","iopub.status.idle":"2023-09-21T14:06:04.868622Z","shell.execute_reply.started":"2023-09-21T14:05:43.889193Z","shell.execute_reply":"2023-09-21T14:06:04.867220Z"},"trusted":true},"execution_count":169,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 0: train:   0%|          | 0/361 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5fa9c0f3a5f411c956aca202850bdc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 0: val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a383d1715db54c09bdecec568941a311"}},"metadata":{}},{"name":"stdout","text":"model saved\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1: train:   0%|          | 0/361 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b15ef157245471998d92d237f00b5bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1: val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff1a979ee83a430892ea086f9bb67468"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2: train:   0%|          | 0/361 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc80fdc0e5c641cfa7f11e27a8f6ec46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2: val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4fdc2e660cb40998d93a12dbf1a7c1d"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Predictions\n\nWrite prediction. That's it. No more instructions, you already made it 3 times.","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\ntorch.manual_seed(420)\nfrom torchtext.vocab import build_vocab_from_iterator\n\n\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe: pd.DataFrame, vocab = None, max_size=100):\n        self.dataframe = dataframe\n        self._preprocess()\n        self.vocab = vocab\n\n    def _preprocess(self):\n        # fill missing values in entities\n        self.dataframe['entity'] = self.dataframe['entity'].fillna(\"other\")\n\n        # Clean entities column\n        self.dataframe['entity'] = self.dataframe['entity'].str.lower()\n        \n        # Split the dataset, so that we will have \n        # full sentences and full tags by the same index\n        self.dataframe.sort_values(by=['sentence_id', 'entity_id'], ascending=[True, True])\n\n        self.sentences = list(self.dataframe.groupby('sentence_id')['entity'].agg(list).reset_index()['entity'])\n        print(np.sum(len(s) for s in self.sentences))\n        \n\n    def _get_sentence(self, index: int) -> list:\n        # retrieves sentence from dataset by index\n        sent = self.sentences[index]\n        return self.vocab(sent)\n\n    def __getitem__(self, index) -> list:\n        return self._get_sentence(index)\n    \n    def __len__(self) -> int:\n        return len(self.sentences)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:06:04.870541Z","iopub.execute_input":"2023-09-21T14:06:04.870971Z","iopub.status.idle":"2023-09-21T14:06:04.886135Z","shell.execute_reply.started":"2023-09-21T14:06:04.870932Z","shell.execute_reply":"2023-09-21T14:06:04.884594Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"# you can use the same dataset class\ntest_dataset = TestDataset(test, vocab=train_dataset.vocab)\nlen(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:06:04.887782Z","iopub.execute_input":"2023-09-21T14:06:04.889025Z","iopub.status.idle":"2023-09-21T14:06:05.553138Z","shell.execute_reply.started":"2023-09-21T14:06:04.888986Z","shell.execute_reply":"2023-09-21T14:06:05.552141Z"},"trusted":true},"execution_count":171,"outputs":[{"name":"stdout","text":"303025\n","output_type":"stream"},{"execution_count":171,"output_type":"execute_result","data":{"text/plain":"14441"},"metadata":{}}]},{"cell_type":"code","source":"batch_size = 128\nmax_size = 128\n\n# remebder that for training we can use pads but for testing we need to write \n# exact length of the sentence into the seubmission\ndef collate_batch(batch: list):\n    sentences_batch, sentences_lengths = [], []\n    for _sent in batch:\n        sentences_lengths.append(len(_sent))\n        _sent = _sent[:max_size]\n        _sent = [PAD_IDX] * ((max_size-len(_sent)) if (len(_sent) < max_size) else 0) + _sent\n        sentences_batch.append(_sent)\n\n    sentences_batch = torch.tensor(sentences_batch).T\n    sentences_lengths = torch.tensor(sentences_lengths)\n    return sentences_batch.to(device), sentences_lengths.to(device)\n\ntest_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:06:05.554786Z","iopub.execute_input":"2023-09-21T14:06:05.555207Z","iopub.status.idle":"2023-09-21T14:06:05.580374Z","shell.execute_reply.started":"2023-09-21T14:06:05.555169Z","shell.execute_reply":"2023-09-21T14:06:05.579251Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"for i, j in test_dataloader:\n    print(i.size(), j.size())\n    break\nprint(len(test_dataloader))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:06:05.581886Z","iopub.execute_input":"2023-09-21T14:06:05.582389Z","iopub.status.idle":"2023-09-21T14:06:05.606454Z","shell.execute_reply.started":"2023-09-21T14:06:05.582353Z","shell.execute_reply":"2023-09-21T14:06:05.605189Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stdout","text":"torch.Size([128, 128]) torch.Size([128])\n113\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict(\n    model,\n    loader,\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Predictions\",\n        leave=True,\n    )\n    predictions = []\n    l = 0\n    with torch.no_grad():\n        model.eval()  # evaluation mode\n        for i, batch in loop:\n            texts, lens = batch\n            l += torch.sum(lens)\n\n            # forward pass and loss calculation\n            outputs = model(texts) # max_size, batch, classes\n            \n            _, predicted = torch.max(outputs, 2)\n            \n            predicted = predicted.permute(1,0).detach().cpu().tolist()\n            \n            for i in range(len(batch[1])):\n                predictions += predicted[i][-lens[i]:]\n                \n            \n#             print(predictions)\n    print(l)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:06:05.608007Z","iopub.execute_input":"2023-09-21T14:06:05.608598Z","iopub.status.idle":"2023-09-21T14:06:05.619384Z","shell.execute_reply.started":"2023-09-21T14:06:05.608563Z","shell.execute_reply":"2023-09-21T14:06:05.618393Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"ckpt = torch.load(\"best.pt\")\nmodel.load_state_dict(ckpt)\n\npredictions = predict(model, test_dataloader)\npredictions[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:06:05.620777Z","iopub.execute_input":"2023-09-21T14:06:05.621262Z","iopub.status.idle":"2023-09-21T14:06:08.225274Z","shell.execute_reply.started":"2023-09-21T14:06:05.621226Z","shell.execute_reply":"2023-09-21T14:06:08.224156Z"},"trusted":true},"execution_count":175,"outputs":[{"output_type":"display_data","data":{"text/plain":"Predictions:   0%|          | 0/113 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80220b0134f24d0ab25fcb23deaf77c2"}},"metadata":{}},{"name":"stdout","text":"tensor(303025, device='cuda:0')\n","output_type":"stream"},{"execution_count":175,"output_type":"execute_result","data":{"text/plain":"[1, 4, 5, 5, 10, 9, 7, 0, 5, 9]"},"metadata":{}}]},{"cell_type":"code","source":"results = pd.Series(predictions).apply(lambda x: idx2cat[x])\nresults.to_csv('submission.csv', index_label='id')\nresults","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:06:08.227351Z","iopub.execute_input":"2023-09-21T14:06:08.228126Z","iopub.status.idle":"2023-09-21T14:06:09.169360Z","shell.execute_reply.started":"2023-09-21T14:06:08.228086Z","shell.execute_reply":"2023-09-21T14:06:09.168403Z"},"trusted":true},"execution_count":176,"outputs":[{"execution_count":176,"output_type":"execute_result","data":{"text/plain":"0          ADP\n1          DET\n2         NOUN\n3         NOUN\n4            .\n          ... \n303020    NOUN\n303021     PRT\n303022    VERB\n303023    NOUN\n303024       .\nLength: 303025, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"len(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:06:09.170729Z","iopub.execute_input":"2023-09-21T14:06:09.171648Z","iopub.status.idle":"2023-09-21T14:06:09.178712Z","shell.execute_reply.started":"2023-09-21T14:06:09.171609Z","shell.execute_reply":"2023-09-21T14:06:09.177630Z"},"trusted":true},"execution_count":177,"outputs":[{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"303025"},"metadata":{}}]},{"cell_type":"code","source":"len(results[results == 'X'])\n# results","metadata":{"execution":{"iopub.status.busy":"2023-09-21T14:06:09.180830Z","iopub.execute_input":"2023-09-21T14:06:09.181449Z","iopub.status.idle":"2023-09-21T14:06:09.239490Z","shell.execute_reply.started":"2023-09-21T14:06:09.181413Z","shell.execute_reply":"2023-09-21T14:06:09.238564Z"},"trusted":true},"execution_count":178,"outputs":[{"execution_count":178,"output_type":"execute_result","data":{"text/plain":"1158"},"metadata":{}}]},{"cell_type":"markdown","source":"[1, 4, 5, 5, 10, 5, 7, 5, 5, 9]\n\n0          ADP\n,1          DET\n,2         NOUN\n,3         NOUN\n,4            .\n,          ... \n,303020    NOUN\n,303021     PRT\n,303022    VERB\n,303023    NOUN\n,303024       .\n,Length: 303025, dtype: object","metadata":{}}]}
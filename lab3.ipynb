{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Practical machine learning and deep learning. Lab 3\n\n# Deep Learning in Natural Language Processing\n\n# [Competition](https://www.kaggle.com/t/4677b08c063f433ba1eb8f3543af90b4)\n\n## Goal\n\nYour goal is to implement Neural Network to classify Amazon Products reviews. \n\n## Submission\n\nSubmission format is described at competition page.","metadata":{}},{"cell_type":"markdown","source":"## Data preprocessing\n\nData preprocessing is an essential step in building a Machine Learning model and depending on how well the data has been preprocessed.\n\nIn NLP, text preprocessing is the first step in the process of building a model.\n\nThe various text preprocessing steps are:\n\n* Tokenization\n* Lower casing\n* Stop words removal\n* Stemming\n* Lemmatization\n\nThese various text preprocessing steps are widely used for dimensionality reduction.\n\nFirst, let's read the input data and then perform preprocessing steps","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntrain_dataframe = pd.read_csv('/kaggle/input/pmldl-week-3-dl-in-natural-language-processing/train.csv')\ntest_dataframe = pd.read_csv('/kaggle/input/pmldl-week-3-dl-in-natural-language-processing/test.csv')\n\ntrain_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:16:50.295652Z","iopub.execute_input":"2023-09-13T14:16:50.296277Z","iopub.status.idle":"2023-09-13T14:16:51.341205Z","shell.execute_reply.started":"2023-09-13T14:16:50.296243Z","shell.execute_reply":"2023-09-13T14:16:51.339852Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"                                 Title Helpfulness  Score  \\\n0  Golden Valley Natural Buffalo Jerky         0/0    3.0   \n1                         Westing Game         0/0    5.0   \n2                         Westing Game         0/0    5.0   \n3                         Westing Game         0/0    5.0   \n4    I SPY A is For Jigsaw Puzzle 63pc         2/4    5.0   \n\n                                                Text              Category  \n0  The description and photo on this product need...  grocery gourmet food  \n1  This was a great book!!!! It is well thought t...            toys games  \n2  I am a first year teacher, teaching 5th grade....            toys games  \n3  I got the book at my bookfair at school lookin...            toys games  \n4  Hi! I'm Martine Redman and I created this puzz...            toys games  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Helpfulness</th>\n      <th>Score</th>\n      <th>Text</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Golden Valley Natural Buffalo Jerky</td>\n      <td>0/0</td>\n      <td>3.0</td>\n      <td>The description and photo on this product need...</td>\n      <td>grocery gourmet food</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Westing Game</td>\n      <td>0/0</td>\n      <td>5.0</td>\n      <td>This was a great book!!!! It is well thought t...</td>\n      <td>toys games</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Westing Game</td>\n      <td>0/0</td>\n      <td>5.0</td>\n      <td>I am a first year teacher, teaching 5th grade....</td>\n      <td>toys games</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Westing Game</td>\n      <td>0/0</td>\n      <td>5.0</td>\n      <td>I got the book at my bookfair at school lookin...</td>\n      <td>toys games</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I SPY A is For Jigsaw Puzzle 63pc</td>\n      <td>2/4</td>\n      <td>5.0</td>\n      <td>Hi! I'm Martine Redman and I created this puzz...</td>\n      <td>toys games</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"In the training data we have `4` features (`Title`, `Helpfulness`, `Score` and `Text`) with target category (`Category`). For the test features are the same, except for target column.\n\nFirst, let's write functions for preprocessing helpfulness and score feature in case we needed them.","metadata":{}},{"cell_type":"code","source":"def preprocess_score_inplace(df):\n    \"\"\"\n    Normalizes score to make it from 0 to 1.\n    \n    For now it is from 1.0 to 5.0, so natural choice\n    is to normalize by (f - 1.0)/4.0\n    \"\"\"\n    df['Score'] = (df['Score'] - 1.0) / 4.0\n    return df\n\ndef preprocess_helpfulness_inplace(df):\n    \"\"\"\n    Splits feature by '/' and normalize helpfulness to make it from 0 to 1\n    \n    The total number of assessments can be 0, so let's substitute it\n    with 1. The resulting helpfulness still will be zero but we\n    remove the possibility of division by zero exception.\n    \"\"\"\n    _splitted = df['Helpfulness'].str.split('/', expand=True)\n    _helpful, _total = _splitted[0], _splitted[1]\n    _total.replace(\"0\", \"1\", inplace=True)\n    df['Helpfulness'] = _helpful.astype(int) / _total.astype(int)\n    return df    ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:08.089113Z","iopub.execute_input":"2023-09-13T14:17:08.090107Z","iopub.status.idle":"2023-09-13T14:17:08.101787Z","shell.execute_reply.started":"2023-09-13T14:17:08.090059Z","shell.execute_reply":"2023-09-13T14:17:08.100372Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The two other features are both text. For simplicity, let's remove concatenate them so that we will have one full text feature. The resulting code is also a function.","metadata":{}},{"cell_type":"code","source":"def concat_title_text_inplace(df):\n    \"\"\"\n    Concatenates Title and Text columns together\n    \"\"\"\n    df['Text'] = df['Title'] + \" \" + df['Text']\n    df.drop('Title', axis=1, inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:09.186125Z","iopub.execute_input":"2023-09-13T14:17:09.186837Z","iopub.status.idle":"2023-09-13T14:17:09.198611Z","shell.execute_reply.started":"2023-09-13T14:17:09.186784Z","shell.execute_reply":"2023-09-13T14:17:09.197409Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Also, encode the target categories, so that the output is become an index","metadata":{}},{"cell_type":"code","source":"# define categories indices\ncat2idx = {\n    'toys games': 0,\n    'health personal care': 1,\n    'beauty': 2,\n    'baby products': 3,\n    'pet supplies': 4,\n    'grocery gourmet food': 5,\n}\n# define reverse mapping\nidx2cat = {\n    v:k for k,v in cat2idx.items()\n}","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:10.892377Z","iopub.execute_input":"2023-09-13T14:17:10.893463Z","iopub.status.idle":"2023-09-13T14:17:10.899857Z","shell.execute_reply.started":"2023-09-13T14:17:10.893424Z","shell.execute_reply":"2023-09-13T14:17:10.898559Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def encode_categories(df):\n    df['Category'] = df['Category'].apply(lambda x: cat2idx[x])\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:12.194417Z","iopub.execute_input":"2023-09-13T14:17:12.194788Z","iopub.status.idle":"2023-09-13T14:17:12.201236Z","shell.execute_reply.started":"2023-09-13T14:17:12.194760Z","shell.execute_reply":"2023-09-13T14:17:12.199793Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize our first stage of preprocessing.","metadata":{}},{"cell_type":"code","source":"train_copy = train_dataframe.head().copy()\n\nencode_categories(preprocess_score_inplace(preprocess_helpfulness_inplace(concat_title_text_inplace(train_copy))))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:13.749147Z","iopub.execute_input":"2023-09-13T14:17:13.749589Z","iopub.status.idle":"2023-09-13T14:17:13.778492Z","shell.execute_reply.started":"2023-09-13T14:17:13.749553Z","shell.execute_reply":"2023-09-13T14:17:13.777214Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Helpfulness  Score                                               Text  \\\n0          0.0    0.5  Golden Valley Natural Buffalo Jerky The descri...   \n1          0.0    1.0  Westing Game This was a great book!!!! It is w...   \n2          0.0    1.0  Westing Game I am a first year teacher, teachi...   \n3          0.0    1.0  Westing Game I got the book at my bookfair at ...   \n4          0.5    1.0  I SPY A is For Jigsaw Puzzle 63pc Hi! I'm Mart...   \n\n   Category  \n0         5  \n1         0  \n2         0  \n3         0  \n4         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Helpfulness</th>\n      <th>Score</th>\n      <th>Text</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>Golden Valley Natural Buffalo Jerky The descri...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Westing Game This was a great book!!!! It is w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Westing Game I am a first year teacher, teachi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>Westing Game I got the book at my bookfair at ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.5</td>\n      <td>1.0</td>\n      <td>I SPY A is For Jigsaw Puzzle 63pc Hi! I'm Mart...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Text cleaning\n\nFor text cleaning, you can use lower casting, punctuation removal, numbers removal, tokenization, stop words removal, stemming. This will get a perfectly cleaned text without any garbage information.","metadata":{}},{"cell_type":"code","source":"import re\n\ndef lower_text(text: str):\n    return text.lower()\n\ndef remove_numbers(text: str):\n    \"\"\"\n    Substitute all punctuations with space in case of\n    \"there is5dogs\".\n    \n    If subs with '' -> \"there isdogs\"\n    With ' ' -> there is dogs\n    \"\"\"\n    text_nonum = re.sub(r'\\d+', ' ', text)\n    return text_nonum\n\ndef remove_punctuation(text: str):\n    \"\"\"\n    Substitute all punctiations with space in case of\n    \"hello!nice to meet you\"\n    \n    If subs with '' -> \"hellonice to meet you\"\n    With ' ' -> \"hello nice to meet you\"\n    \"\"\"\n    text_nopunct = re.sub(r'[^a-z|\\s]+', ' ', text)\n    return text_nopunct\n\ndef remove_multiple_spaces(text: str):\n    text_no_doublespace = re.sub('\\s+', ' ', text).strip()\n    return text_no_doublespace","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:15.157322Z","iopub.execute_input":"2023-09-13T14:17:15.157818Z","iopub.status.idle":"2023-09-13T14:17:15.168374Z","shell.execute_reply.started":"2023-09-13T14:17:15.157775Z","shell.execute_reply":"2023-09-13T14:17:15.166880Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"This will give us clean text.","metadata":{}},{"cell_type":"code","source":"sample_text = train_copy['Text'][4]\n\n_lowered = lower_text(sample_text)\n_without_numbers = remove_numbers(_lowered)\n_without_punct = remove_punctuation(_without_numbers)\n_single_spaced = remove_multiple_spaces(_without_punct)\n\nprint(sample_text)\nprint('-'*10)\nprint(_lowered)\nprint('-'*10)\nprint(_without_numbers)\nprint('-'*10)\nprint(_without_punct)\nprint('-'*10)\nprint(_single_spaced)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:16.501596Z","iopub.execute_input":"2023-09-13T14:17:16.502002Z","iopub.status.idle":"2023-09-13T14:17:16.510443Z","shell.execute_reply.started":"2023-09-13T14:17:16.501969Z","shell.execute_reply":"2023-09-13T14:17:16.509434Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"I SPY A is For Jigsaw Puzzle 63pc Hi! I'm Martine Redman and I created this puzzle for Briarpatch using a great photo from Jean Marzollo and Walter Wick's terrific book, I Spy School Days. Kids need lots of practice to master the ABC's, and this puzzle provides an enjoyable reinforcing tool. Its visual richness helps non-readers and readers alike to remember word associations, and the wealth of cleverly chosen objects surrounding each letter promote language development. The riddle included multiplies the fun of assembling this colorful puzzle. For another great Briarpatch puzzle, check out I Spy Blocks. END\n----------\ni spy a is for jigsaw puzzle 63pc hi! i'm martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick's terrific book, i spy school days. kids need lots of practice to master the abc's, and this puzzle provides an enjoyable reinforcing tool. its visual richness helps non-readers and readers alike to remember word associations, and the wealth of cleverly chosen objects surrounding each letter promote language development. the riddle included multiplies the fun of assembling this colorful puzzle. for another great briarpatch puzzle, check out i spy blocks. end\n----------\ni spy a is for jigsaw puzzle  pc hi! i'm martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick's terrific book, i spy school days. kids need lots of practice to master the abc's, and this puzzle provides an enjoyable reinforcing tool. its visual richness helps non-readers and readers alike to remember word associations, and the wealth of cleverly chosen objects surrounding each letter promote language development. the riddle included multiplies the fun of assembling this colorful puzzle. for another great briarpatch puzzle, check out i spy blocks. end\n----------\ni spy a is for jigsaw puzzle  pc hi  i m martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick s terrific book  i spy school days  kids need lots of practice to master the abc s  and this puzzle provides an enjoyable reinforcing tool  its visual richness helps non readers and readers alike to remember word associations  and the wealth of cleverly chosen objects surrounding each letter promote language development  the riddle included multiplies the fun of assembling this colorful puzzle  for another great briarpatch puzzle  check out i spy blocks  end\n----------\ni spy a is for jigsaw puzzle pc hi i m martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick s terrific book i spy school days kids need lots of practice to master the abc s and this puzzle provides an enjoyable reinforcing tool its visual richness helps non readers and readers alike to remember word associations and the wealth of cleverly chosen objects surrounding each letter promote language development the riddle included multiplies the fun of assembling this colorful puzzle for another great briarpatch puzzle check out i spy blocks end\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now, harder preprocessing: tokenization, stop words removal and stemming.\nFor that you can use several packages, but we encourage you to use `nltk` - Natural Language ToolKit as well as `torchtext`.\n\n\nTake a look at:\n* `nltk.tokenize.word_tokenize` or `torchtext.data.utils.get_tokenizer` for tokenization\n* `nltk.corpus.stopwords` for stop words removal\n* `nltk.stem.PorterStemmer` for stemming","metadata":{}},{"cell_type":"code","source":"# imports here\nimport nltk\nimport torchtext\n\ndef tokenize_text(text: str) -> list[str]:\n    return nltk.tokenize.word_tokenize(text)\n\ndef remove_stop_words(tokenized_text: list[str]) -> list[str]:\n    stop_words = set(nltk.corpus.stopwords.words('english'))\n    return [w for w in tokenized_text if not w in stop_words]\n\ndef stem_words(tokenized_text: list[str]) -> list[str]:\n    stemmer = nltk.stem.PorterStemmer()\n    return [stemmer.stem(word) for word in tokenized_text]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:17.842017Z","iopub.execute_input":"2023-09-13T14:17:17.842696Z","iopub.status.idle":"2023-09-13T14:17:23.817720Z","shell.execute_reply.started":"2023-09-13T14:17:17.842653Z","shell.execute_reply":"2023-09-13T14:17:23.816391Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"_tokenized = tokenize_text(_single_spaced)\n_without_sw = remove_stop_words(_tokenized)\n_stemmed = stem_words(_without_sw)\n\nprint(_single_spaced)\nprint('-'*10)\nprint(_tokenized)\nprint('-'*10)\nprint(_without_sw)\nprint('-'*10)\nprint(_stemmed)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:23.822756Z","iopub.execute_input":"2023-09-13T14:17:23.823394Z","iopub.status.idle":"2023-09-13T14:17:23.854349Z","shell.execute_reply.started":"2023-09-13T14:17:23.823352Z","shell.execute_reply":"2023-09-13T14:17:23.853387Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"i spy a is for jigsaw puzzle pc hi i m martine redman and i created this puzzle for briarpatch using a great photo from jean marzollo and walter wick s terrific book i spy school days kids need lots of practice to master the abc s and this puzzle provides an enjoyable reinforcing tool its visual richness helps non readers and readers alike to remember word associations and the wealth of cleverly chosen objects surrounding each letter promote language development the riddle included multiplies the fun of assembling this colorful puzzle for another great briarpatch puzzle check out i spy blocks end\n----------\n['i', 'spy', 'a', 'is', 'for', 'jigsaw', 'puzzle', 'pc', 'hi', 'i', 'm', 'martine', 'redman', 'and', 'i', 'created', 'this', 'puzzle', 'for', 'briarpatch', 'using', 'a', 'great', 'photo', 'from', 'jean', 'marzollo', 'and', 'walter', 'wick', 's', 'terrific', 'book', 'i', 'spy', 'school', 'days', 'kids', 'need', 'lots', 'of', 'practice', 'to', 'master', 'the', 'abc', 's', 'and', 'this', 'puzzle', 'provides', 'an', 'enjoyable', 'reinforcing', 'tool', 'its', 'visual', 'richness', 'helps', 'non', 'readers', 'and', 'readers', 'alike', 'to', 'remember', 'word', 'associations', 'and', 'the', 'wealth', 'of', 'cleverly', 'chosen', 'objects', 'surrounding', 'each', 'letter', 'promote', 'language', 'development', 'the', 'riddle', 'included', 'multiplies', 'the', 'fun', 'of', 'assembling', 'this', 'colorful', 'puzzle', 'for', 'another', 'great', 'briarpatch', 'puzzle', 'check', 'out', 'i', 'spy', 'blocks', 'end']\n----------\n['spy', 'jigsaw', 'puzzle', 'pc', 'hi', 'martine', 'redman', 'created', 'puzzle', 'briarpatch', 'using', 'great', 'photo', 'jean', 'marzollo', 'walter', 'wick', 'terrific', 'book', 'spy', 'school', 'days', 'kids', 'need', 'lots', 'practice', 'master', 'abc', 'puzzle', 'provides', 'enjoyable', 'reinforcing', 'tool', 'visual', 'richness', 'helps', 'non', 'readers', 'readers', 'alike', 'remember', 'word', 'associations', 'wealth', 'cleverly', 'chosen', 'objects', 'surrounding', 'letter', 'promote', 'language', 'development', 'riddle', 'included', 'multiplies', 'fun', 'assembling', 'colorful', 'puzzle', 'another', 'great', 'briarpatch', 'puzzle', 'check', 'spy', 'blocks', 'end']\n----------\n['spi', 'jigsaw', 'puzzl', 'pc', 'hi', 'martin', 'redman', 'creat', 'puzzl', 'briarpatch', 'use', 'great', 'photo', 'jean', 'marzollo', 'walter', 'wick', 'terrif', 'book', 'spi', 'school', 'day', 'kid', 'need', 'lot', 'practic', 'master', 'abc', 'puzzl', 'provid', 'enjoy', 'reinforc', 'tool', 'visual', 'rich', 'help', 'non', 'reader', 'reader', 'alik', 'rememb', 'word', 'associ', 'wealth', 'cleverli', 'chosen', 'object', 'surround', 'letter', 'promot', 'languag', 'develop', 'riddl', 'includ', 'multipli', 'fun', 'assembl', 'color', 'puzzl', 'anoth', 'great', 'briarpatch', 'puzzl', 'check', 'spi', 'block', 'end']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see, there is a lot of words removed as well as the unnecessary language rules (I mean stems, com'on). Now we are able to construct full cleaning preprocessing stage.","metadata":{}},{"cell_type":"code","source":"def preprocessing_stage(text):\n    _lowered = lower_text(text)\n    _without_numbers = remove_numbers(_lowered)\n    _without_punct = remove_punctuation(_without_numbers)\n    _single_spaced = remove_multiple_spaces(_without_punct)\n    _tokenized = tokenize_text(_single_spaced)\n    _without_sw = remove_stop_words(_tokenized)\n    _stemmed = stem_words(_without_sw)\n    \n    return _stemmed\n\ndef clean_text_inplace(df):\n    df['Text'] = df['Text'].apply(preprocessing_stage)\n    return df\n\ndef preprocess(df):\n    df.fillna(\" \", inplace=True)\n    _preprocess_score = preprocess_score_inplace(df)\n    _preprocess_helpfulness = preprocess_helpfulness_inplace(_preprocess_score)\n    _concatted = concat_title_text_inplace(_preprocess_helpfulness)\n\n    if 'Category' in df.columns:\n        _encoded = encode_categories(_concatted)\n        _cleaned = clean_text_inplace(_encoded)\n    else:\n        _cleaned = clean_text_inplace(_concatted)\n    return _cleaned\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:25.666039Z","iopub.execute_input":"2023-09-13T14:17:25.666731Z","iopub.status.idle":"2023-09-13T14:17:25.676341Z","shell.execute_reply.started":"2023-09-13T14:17:25.666699Z","shell.execute_reply":"2023-09-13T14:17:25.674782Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"And now let's apply it on our train and test dataframes.","metadata":{}},{"cell_type":"code","source":"train_preprocessed = preprocess(train_dataframe)\ntest_preprocessed = preprocess(test_dataframe)\n\ntrain_preprocessed.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:17:26.900522Z","iopub.execute_input":"2023-09-13T14:17:26.900953Z","iopub.status.idle":"2023-09-13T14:19:53.968521Z","shell.execute_reply.started":"2023-09-13T14:17:26.900916Z","shell.execute_reply":"2023-09-13T14:19:53.967218Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   Helpfulness  Score                                               Text  \\\n0          0.0    0.5  [golden, valley, natur, buffalo, jerki, descri...   \n1          0.0    1.0  [west, game, great, book, well, thought, easil...   \n2          0.0    1.0  [west, game, first, year, teacher, teach, th, ...   \n3          0.0    1.0  [west, game, got, book, bookfair, school, look...   \n4          0.5    1.0  [spi, jigsaw, puzzl, pc, hi, martin, redman, c...   \n\n   Category  \n0         5  \n1         0  \n2         0  \n3         0  \n4         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Helpfulness</th>\n      <th>Score</th>\n      <th>Text</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>[golden, valley, natur, buffalo, jerki, descri...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>[west, game, great, book, well, thought, easil...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>[west, game, first, year, teacher, teach, th, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>[west, game, got, book, bookfair, school, look...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.5</td>\n      <td>1.0</td>\n      <td>[spi, jigsaw, puzzl, pc, hi, martin, redman, c...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now, let's split our original train dataset into train and val sets.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nratio = 0.2\ntrain, val = train_test_split(\n    train_preprocessed, stratify=train_preprocessed['Category'], test_size=ratio, random_state=420\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:19:53.971382Z","iopub.execute_input":"2023-09-13T14:19:53.971876Z","iopub.status.idle":"2023-09-13T14:19:54.012829Z","shell.execute_reply.started":"2023-09-13T14:19:53.971826Z","shell.execute_reply":"2023-09-13T14:19:54.011421Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"And now, for the best result, lets get rid of pandas so that nothing is stopping us from working with torchtext. For that let's create an iterator that is going to yield samples for us.","metadata":{}},{"cell_type":"markdown","source":"# Creating dataloaders\n\nFirst, you should generate our vocab from the train set.\n\nFor that, use `torchtext.vocab.build_vocab_from_iterator`.","metadata":{}},{"cell_type":"code","source":"from torchtext.vocab import build_vocab_from_iterator\n\ndef yield_tokens(train):\n    for _, sample in train.iterrows():\n        yield sample.to_list()[2]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:19:54.014532Z","iopub.execute_input":"2023-09-13T14:19:54.014889Z","iopub.status.idle":"2023-09-13T14:19:54.020766Z","shell.execute_reply.started":"2023-09-13T14:19:54.014859Z","shell.execute_reply":"2023-09-13T14:19:54.019547Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define special symbols and indices\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n\nvocab = build_vocab_from_iterator(yield_tokens(train), specials=special_symbols)\nvocab.set_default_index(UNK_IDX)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:19:54.023853Z","iopub.execute_input":"2023-09-13T14:19:54.024232Z","iopub.status.idle":"2023-09-13T14:19:56.918065Z","shell.execute_reply.started":"2023-09-13T14:19:54.024180Z","shell.execute_reply":"2023-09-13T14:19:56.916972Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"And then use our vocab to encode the tokenized sequence","metadata":{}},{"cell_type":"code","source":"sample = train['Text'][2]\nprint(sample)\nencoded = vocab(sample)\nprint(encoded)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:19:56.919626Z","iopub.execute_input":"2023-09-13T14:19:56.919943Z","iopub.status.idle":"2023-09-13T14:19:56.928581Z","shell.execute_reply.started":"2023-09-13T14:19:56.919917Z","shell.execute_reply":"2023-09-13T14:19:56.927326Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"['west', 'game', 'first', 'year', 'teacher', 'teach', 'th', 'grade', 'special', 'read', 'class', 'high', 'comprehens', 'level', 'read', 'book', 'one', 'best', 'thing', 'taught', 'year', 'expand', 'mind', 'allow', 'put', 'charact', 'place', 'easi', 'student', 'make', 'mind', 'movi', 'even', 'use', 'whole', 'read', 'class', 'time', 'order', 'finish', 'book', 'student', 'wait', 'hear', 'end', 'excel', 'book', 'read', 'everi', 'year', 'student']\n[2556, 43, 33, 14, 2751, 807, 860, 1724, 728, 131, 1895, 191, 6981, 583, 131, 515, 5, 59, 46, 3505, 14, 2954, 528, 450, 40, 1125, 165, 50, 1924, 22, 528, 945, 30, 4, 271, 131, 1895, 13, 68, 623, 515, 1924, 426, 600, 180, 311, 515, 131, 85, 14, 1924]\n","output_type":"stream"}]},{"cell_type":"code","source":"# import numpy as np\n# max_sentence_len = np.max([len(list(train['Text'])[i]) for i in range(len(train))])\n# max_sentence_len","metadata":{"execution":{"iopub.status.busy":"2023-09-13T12:26:09.457764Z","iopub.execute_input":"2023-09-13T12:26:09.458152Z","iopub.status.idle":"2023-09-13T12:29:24.531673Z","shell.execute_reply.started":"2023-09-13T12:26:09.458122Z","shell.execute_reply":"2023-09-13T12:29:24.530559Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"538"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we can define our collate function and create dataloaders","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\ntorch.manual_seed(420)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef collate_batch(batch):\n    label_list, text_list, score_list, helpfulness_list, offsets = [], [], [], [], [0]\n    for _helpfulnes, _score, _text, _label in batch:\n        label_list.append(_label)\n        text = torch.tensor(vocab(_text), dtype=torch.int64)\n        text_list.append(text)\n        score_list.append(_score)\n        helpfulness_list.append(_helpfulnes)\n        offsets.append(text.size(0))\n        \n    \n    label_list = torch.tensor(label_list)\n    text_list = torch.cat(text_list)\n    score_list = torch.tensor(score_list)\n    helpfulness_list = torch.tensor(helpfulness_list)\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n\n    return label_list.to(device), text_list.to(device), offsets.to(device), score_list.to(device), helpfulness_list.to(device)\n\ntrain_dataloader = DataLoader(\n    train.to_numpy(), batch_size=128, shuffle=True, collate_fn=collate_batch\n)\n\nval_dataloader = DataLoader(\n    val.to_numpy(), batch_size=128, shuffle=False, collate_fn=collate_batch\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:19:56.930543Z","iopub.execute_input":"2023-09-13T14:19:56.930943Z","iopub.status.idle":"2023-09-13T14:19:56.956919Z","shell.execute_reply.started":"2023-09-13T14:19:56.930879Z","shell.execute_reply":"2023-09-13T14:19:56.955816Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Defining Network\n\n\nFor writing a network you can use `torch.nn.Embedding` or `torch.nn.EmbeddingBag`. This will allow your netorwk to learn embedding vector for your tokens.\n\nAs for the other modules in your network, consider these options:\n* Simple Linear layers, activations, basic stuff that goes into the network\n* There is a possible of not using the offsets (indices of sequences) in the formard, put use predefined sequence length (maximum length, some value, etc.). If this is an option for you, change the `collate_batch` function according to your architecture.\n* You could use all this recurrent stuff (RNN, GRU, LSTM, even Transformer, all up to you), but remembder about the dimentions and hidden states\n* If you have any quiestions - google it","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass TextClassificationModel(nn.Module):\n    def __init__(self, num_classes):\n        super(TextClassificationModel, self).__init__()\n        self.embedding = nn.EmbeddingBag(len(vocab), 64, sparse=False)\n        self.linear = nn.Linear(64, num_classes)\n    \n\n    def forward(self, text, offsets):\n        \n        embedded = self.embedding(text, offsets)\n        output = self.linear(embedded)\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:19:56.958945Z","iopub.execute_input":"2023-09-13T14:19:56.959612Z","iopub.status.idle":"2023-09-13T14:19:56.969724Z","shell.execute_reply.started":"2023-09-13T14:19:56.959570Z","shell.execute_reply":"2023-09-13T14:19:56.968419Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\nfrom tqdm.autonotebook import tqdm\n\ndef train_one_epoch(\n    model,\n    loader,\n    optimizer,\n    loss_fn,\n    epoch_num=-1\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Epoch {epoch_num}: train\",\n        leave=True,\n    )\n    model.train()\n    train_loss = 0.0\n    for i, batch in loop:\n        labels, texts, offsets, scores, helpfulness = batch\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward pass\n        outputs = model(texts, offsets)\n        # loss calculation\n        loss = loss_fn(outputs, labels)\n        \n        # backward pass\n        loss.backward()\n\n        # optimizer run\n        optimizer.step()\n\n        train_loss += loss.item()\n        loop.set_postfix({\"loss\": train_loss/(i * len(labels))})\n\ndef val_one_epoch(\n    model,\n    loader,\n    loss_fn,\n    epoch_num=-1,\n    best_so_far=0.0,\n    ckpt_path='best.pt'\n):\n    \n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=f\"Epoch {epoch_num}: val\",\n        leave=True,\n    )\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        model.eval()  # evaluation mode\n        for i, batch in loop:\n            labels, texts, offsets, scores, helpfulness = batch\n\n            # forward pass\n            outputs = model(texts, offsets)\n            # loss calculation\n            loss = loss_fn(outputs, labels)\n        \n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted==labels).sum().item()\n\n            val_loss += loss.item()\n            loop.set_postfix({\"loss\": val_loss/total, \"acc\": correct / total})\n\n        if correct / total > best:\n            torch.save(model.state_dict(), ckpt_path)\n            return correct / total\n\n    return best_so_far","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:19:56.971461Z","iopub.execute_input":"2023-09-13T14:19:56.971855Z","iopub.status.idle":"2023-09-13T14:19:56.986510Z","shell.execute_reply.started":"2023-09-13T14:19:56.971825Z","shell.execute_reply":"2023-09-13T14:19:56.985440Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nepochs = 10\nmodel = TextClassificationModel(6).to(device)\noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:19:56.988009Z","iopub.execute_input":"2023-09-13T14:19:56.988590Z","iopub.status.idle":"2023-09-13T14:19:57.081777Z","shell.execute_reply.started":"2023-09-13T14:19:56.988553Z","shell.execute_reply":"2023-09-13T14:19:57.079630Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"%matplotlib widget\nbest = -float('inf')\nfor epoch in range(epochs):\n    train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch_num=epoch)\n    best = val_one_epoch(model, val_dataloader, loss_fn, epoch, best_so_far=best)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:19:57.086629Z","iopub.execute_input":"2023-09-13T14:19:57.087007Z","iopub.status.idle":"2023-09-13T14:20:58.993858Z","shell.execute_reply.started":"2023-09-13T14:19:57.086978Z","shell.execute_reply":"2023-09-13T14:20:58.992561Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 0: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a932e4461f8b41db85fe6cdc0543ee81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 0: val:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf524a32fa34e1993a7a30d2f99b320"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bf6de63477d4378879f673ea9b2f57d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1: val:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84a15eab05a640be815d3ddc879a5b84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66c116574ab94d14aee59e40ed58e89e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2: val:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c94f7641aad34878ae63597b3d42a8f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b7cc8a26588401f9e97d55a89046eea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3: val:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dc99589edb64f9692cc0bd57ff2b119"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e21da7fb02d451e88614e3dd14faa36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4: val:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b9f33b2dd314a4b8b153f20f2f2c7a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 5: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45ffbc7298f4fe286e9edb19c111c6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 5: val:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32e4307cb03944a891296f4a32871a0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 6: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"285585ee4a464297ad06d31985555f04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 6: val:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df8eb11113842ceb08d75cf0fe516bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 7: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e32bc58958ef45b6a7c81a27024cef64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 7: val:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"556e9b35ca194cb199cd22e08a6d9ac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 8: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa964858d8574681b3c729641c232eae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 8: val:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2a915947ca421cac7e2b7b3084a4b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 9: train:   0%|          | 0/250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a24123e9154497811067c46fa32f63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 9: val:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdf6157471304564a01228b5453f7ae3"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Predictions","metadata":{}},{"cell_type":"code","source":"def collate_batch(batch):\n    text_list, score_list, helpfulness_list, offsets = [], [], [], [0]\n    for _, _helpfulnes, _score, _text in batch:\n        text = torch.tensor(vocab(_text), dtype=torch.int64)\n        text_list.append(text)\n        score_list.append(_score)\n        helpfulness_list.append(_helpfulnes)\n        offsets.append(text.size(0))\n        \n    text_list = torch.cat(text_list)\n    score_list = torch.tensor(score_list)\n    helpfulness_list = torch.tensor(helpfulness_list)\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n\n    return text_list.to(device), offsets.to(device), score_list.to(device), helpfulness_list.to(device)\n\n\ntest_dataloader = DataLoader(\n    test_preprocessed.to_numpy(), batch_size=128, shuffle=False, collate_fn=collate_batch\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:20:58.995240Z","iopub.execute_input":"2023-09-13T14:20:58.995600Z","iopub.status.idle":"2023-09-13T14:20:59.009758Z","shell.execute_reply.started":"2023-09-13T14:20:58.995570Z","shell.execute_reply":"2023-09-13T14:20:59.008351Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def predict(\n    model,\n    loader,\n):\n    loop = tqdm(\n        enumerate(loader, 1),\n        total=len(loader),\n        desc=\"Predictions:\",\n        leave=True,\n    )\n    predictions = []\n    with torch.no_grad():\n        model.eval()  # evaluation mode\n        for i, batch in loop:\n            texts, offsets, scores, helpfulness = batch\n\n            # forward pass and loss calculation\n            outputs = model(texts, offsets)\n            \n            _, predicted = torch.max(outputs.data, 1)\n            predictions += predicted.detach().cpu().tolist()\n\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:20:59.011591Z","iopub.execute_input":"2023-09-13T14:20:59.012351Z","iopub.status.idle":"2023-09-13T14:20:59.022264Z","shell.execute_reply.started":"2023-09-13T14:20:59.012285Z","shell.execute_reply":"2023-09-13T14:20:59.020798Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"ckpt = torch.load(\"best.pt\")\nmodel.load_state_dict(ckpt)\n\npredictions = predict(model, test_dataloader)\npredictions[:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:20:59.023868Z","iopub.execute_input":"2023-09-13T14:20:59.024226Z","iopub.status.idle":"2023-09-13T14:20:59.448993Z","shell.execute_reply.started":"2023-09-13T14:20:59.024194Z","shell.execute_reply":"2023-09-13T14:20:59.448157Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Predictions::   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0af44bdd5a80403a9cbda701f1290f43"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[4, 4, 0, 5, 4, 4, 2, 0, 2, 2]"},"metadata":{}}]},{"cell_type":"markdown","source":"# 4 4 1 5 4 4 2 0 2 2","metadata":{}},{"cell_type":"code","source":"results = pd.Series(predictions).apply(lambda x: idx2cat[x])\nresults.to_csv('submission.csv', index_label='id')","metadata":{"execution":{"iopub.status.busy":"2023-09-13T14:21:16.350951Z","iopub.execute_input":"2023-09-13T14:21:16.351393Z","iopub.status.idle":"2023-09-13T14:21:16.401107Z","shell.execute_reply.started":"2023-09-13T14:21:16.351359Z","shell.execute_reply":"2023-09-13T14:21:16.400053Z"},"trusted":true},"execution_count":28,"outputs":[]}]}